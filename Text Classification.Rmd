---
title: "Text Classification Using Tidy Data"
output: html_notebook
---

This workbook is baed on the work of [Julia Silge](https://juliasilge.com/blog/tidy-text-classification/)

## Get Data

Follow the tutorial we will use the same books 

```{r library}
# Load library
library(tidyverse) # for tidy data
library(gutenbergr) # for getting books
library(tidytext) # for tidy text
library(rsample) # for sampling and splitting data
library(glmnet) # for classification
#library(doMC) # for parallel processing
library(broom) # for anlaysing models
library(yardstick) # for model performance metrics
```

```{r getData}
# titles of the books we are going to use
titles <- c(
  "The War of the Worlds",
  "Pride and Prejudice"
)

# get books
books <- gutenberg_works(title %in% titles) %>%
  gutenberg_download(meta_fields = "title") %>%
  mutate(document = row_number())

# check we have the books
books
```

## Transform to Tidydata

Let’s train a model that can take an individual line and give us a probability that this book comes from Pride and Prejudice vs. from The War of the Worlds.

```{r tidyData}
# convert books in to tidy data
tidy_books <- books %>%
  unnest_tokens(word, text) %>%
  group_by(word) %>%
  filter(n() > 10) %>% # removed the rarest words
  ungroup()

tidy_books
```

## Data Analysis

Some brief data analysis looking at the most frequent words in each book after removing 
stop words

```{r dataAnalysis}
# get the most frequent words for each book

# had to modify the code as there were functions that are not in the latest stabel
# version in CRAN but are in the developer version

tidy_books %>%
  count(title, word, sort = TRUE) %>%
  anti_join(get_stopwords()) %>%
  group_by(title) %>%
  top_n(20) %>%
  ungroup() %>%
  mutate(word = fct_reorder(word, n)) %>%
  ggplot(aes(word, n, fill = title
  )) +
  geom_col(alpha = 0.8, show.legend = FALSE) +
  #scale_x_reordered() +
  coord_flip() +
  facet_wrap(~title, scales = "free") +
  scale_y_continuous(expand = c(0, 0)) +
  labs(
    x = NULL, y = "Word count",
    title = "Most frequent words after removing stop words",
    subtitle = "Words like 'said' occupy similar ranks but other words are quite different"
  )
```

## Machine Learnign Model

### Split Data

We want to split our data into training and testing sets, to use for building the model and evaluating the model. 

```{r split}
# Split data into training and test sets
books_split <- books %>%
  select(document) %>%
  initial_split()

# training and test data
train_data <- training(books_split)
test_data <- testing(books_split)
```

### Sparse Matrix
Transform our training data from a tidy data structure to a sparse matrix to use for our machine learning algorithm.

```{r sparse}
# transform data in to sparse data matrix
sparse_words <- tidy_books %>%
  count(document, word) %>%
  inner_join(train_data) %>%
  cast_sparse(document, word, n)

class(sparse_words)
```

```{r}
dim(sparse_words)
```

You could at this point cbind() other columns, such as non-text numeric data, onto this sparse matrix. Then you can use this combination of text and non-text data as your predictors in the machine learning algorithm, and the regularized regression algorithm we are going to use will find which are important for your problem space.

### Response Variable

We also need to build a dataframe with a response variable to associate each of the rownames() of the sparse matrix with a title, to use as the quantity we will predict in the model.

```{r respVar}
# Get the row indexs for the training data
word_rownames <- as.integer(rownames(sparse_words))

# attach the book names to the row index so we know which book is assoicated with 
# each word
books_joined <- data_frame(document = word_rownames) %>%
  left_join(books %>%
    select(document, title))
```

### Classification Model

Use the glmnet package to fit a logistic regression model with LASSO regularization. It’s a great fit for text classification because the variable selection that LASSO regularization performs can tell you which words are important for your prediction problem.

Can't install the parallel processing function so

```{r model}
# response variable for classification
is_jane <- books_joined$title == "Pride and Prejudice"

# model
model <- cv.glmnet(sparse_words, is_jane,
  family = "binomial",
  parallel = FALSE, keep = TRUE
)
```

```{r}
# Plot model output
plot(model)

plot(model$glmnet.fit)
```

## Model Evaluation

What predictors are driving the model? Let’s use broom to check out the coefficients of the model, for the largest value of lambda with error within 1 standard error of the minimum.

```{r}
# get the coefficitents
coefs <- model$glmnet.fit %>%
  tidy() %>%
  filter(lambda == model$lambda.1se)
```

Which coefficents are the largest in size, in each direction?

```{r}
# plot most likely words that are likely to split the boods
coefs %>%
  group_by(estimate > 0) %>%
  top_n(10, abs(estimate)) %>%
  ungroup() %>%
  ggplot(aes(fct_reorder(term, estimate), estimate, fill = estimate > 0)) +
  geom_col(alpha = 0.8, show.legend = FALSE) +
  coord_flip() +
  labs(
    x = NULL,
    title = "Coefficients that increase/decrease probability the most",
    subtitle = "A document mentioning Martians is unlikely to be written by Jane Austen"
  )
```

### Estimate on Test Data

Let’s create a dataframe that tells us, for each document in the test set, the probability of being written by Jane Austen.

```{r}
intercept <- coefs %>%
  filter(term == "(Intercept)") %>%
  pull(estimate)

classifications <- tidy_books %>%
  inner_join(test_data) %>%
  inner_join(coefs, by = c("word" = "term")) %>%
  group_by(document) %>%
  summarize(score = sum(estimate)) %>%
  mutate(probability = plogis(intercept + score))

classifications

```

### Performance Metrics

```{r}
comment_classes <- classifications %>%
  left_join(books %>%
    select(title, document), by = "document") %>%
  mutate(title = as.factor(title))

# Get AUC
comment_classes %>%
  roc_auc(title, probability)

# Plot ROC Curve
## This doesn't seem to work because of the roc_cruve function
#comment_classes %>%
#  roc_curve(title, probability) %>%
#  ggplot(aes(x = 1 - specificity, y = sensitivity)) +
#  geom_line(
#    color = "midnightblue",
#    size = 1.5
#  ) +
#  geom_abline(
#    lty = 2, alpha = 0.5,
#    color = "gray50",
#    size = 1.2
#  ) +
#  labs(
#    title = "ROC curve for text classification using regularized regression",
#    subtitle = "Predicting whether text was written by Jane Austen or H.G. Wells"
#  )
```

Get the confusion matrix

```{r}
comment_classes %>%
  mutate(
    prediction = case_when(
      probability > 0.5 ~ "Pride and Prejudice",
      TRUE ~ "The War of the Worlds"
    ),
    prediction = as.factor(prediction)
  ) %>%
  conf_mat(title, prediction)
```

### Missclassification
It’s usually worth my while to understand a bit about both false negatives and false positives for my models. Which documents here were incorrectly predicted to be written by Jane Austen, at the extreme probability end?

```{r}
comment_classes %>%
  filter(
    probability > .8,
    title == "The War of the Worlds"
  ) %>%
  sample_n(10) %>%
  inner_join(books %>%
    select(document, text)) %>%
  select(probability, text)
```

Which documents here were incorrectly predicted to not be written by Jane Austen?

```{r}
comment_classes %>%
  filter(
    probability < .3,
    title == "Pride and Prejudice"
  ) %>%
  sample_n(10) %>%
  inner_join(books %>%
    select(document, text)) %>%
  select(probability, text)
```

